# backend/scraper/base_scraper.py
import requests
from bs4 import BeautifulSoup
import pandas as pd
from pathlib import Path

class BaseScraper:
    def __init__(self, config: dict):
        self.site_name = config.get("site_name")
        self.url = config.get("target_url")
        self.selectors = config.get("selectors", {})
        self.storage = config.get("storage", {})
        print(f"ğŸ•·ï¸ åˆå§‹åŒ–çˆ¬èŸ²: {self.site_name} ({self.url})")

    def fetch_page(self):
        """æŠ“å– HTML é é¢"""
        try:
            print(f"ğŸ” æ­£åœ¨è«‹æ±‚é é¢: {self.url}")
            response = requests.get(self.url, timeout=10)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"âŒ è«‹æ±‚å¤±æ•—: {e}")
            return None

    def parse_page(self, html):
        """è§£æ HTML ä¸¦æ“·å–è³‡æ–™"""
        soup = BeautifulSoup(html, "html.parser")
        titles = [a.get_text(strip=True) for a in soup.select(self.selectors.get("title", ""))]
        prices = [p.get_text(strip=True) for p in soup.select(self.selectors.get("price", ""))]
        
        data = []
        for t, p in zip(titles, prices):
            data.append({"title": t, "price": p})
        return data

    def save_to_csv(self, data):
        """å°‡çµæœå„²å­˜ç‚º CSV"""
        path = Path(self.storage.get("path", "output.csv"))
        df = pd.DataFrame(data)
        path.parent.mkdir(parents=True, exist_ok=True)
        df.to_csv(path, index=False, encoding="utf-8-sig")
        print(f"ğŸ’¾ å·²å„²å­˜è³‡æ–™: {path}")

    def run(self):
        """å®Œæ•´åŸ·è¡Œæµç¨‹"""
        html = self.fetch_page()
        if html:
            data = self.parse_page(html)
            self.save_to_csv(data)
            print(f"âœ… {self.site_name} æŠ“å–å®Œæˆï¼Œå…± {len(data)} ç­†è³‡æ–™ã€‚")
